{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pdfplumber langdetect pandas\n",
    "# ! pip install fasttext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = pdf_path=\"/Users/manishmaurya/Documents/Projects_galaxy/Mismatch language/47907491_BG_Spn _Cont.pdf\"\n",
    "\n",
    "# Example usage\n",
    "pdf_path = pdf_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "# from langdetect import detect\n",
    "# from collections import Counter\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # Function to validate a paragraph based on length and punctuation\n",
    "# def is_valid_paragraph(text):\n",
    "#     return (\n",
    "#         len(text.split()) >= 8 and  # Minimum of 8 words\n",
    "#         any(p in text for p in ['.', ':', ';', '!', '?'])  # End with punctuation\n",
    "#     )\n",
    "\n",
    "# # Function to handle multi-column layout\n",
    "# def extract_text_by_columns(page, column_split=300):\n",
    "#     blocks = page.get_text(\"blocks\")\n",
    "#     left_col, right_col = [], []\n",
    "\n",
    "#     for b in blocks:\n",
    "#         x0, y0, x1, y1, text, *_ = b\n",
    "#         if x0 < column_split:\n",
    "#             left_col.append((y0, text))\n",
    "#         else:\n",
    "#             right_col.append((y0, text))\n",
    "\n",
    "#     left_col.sort()\n",
    "#     right_col.sort()\n",
    "#     combined_text = '\\n'.join([t for _, t in left_col + right_col])\n",
    "#     return combined_text\n",
    "\n",
    "# # Extract paragraphs from the PDF\n",
    "# def extract_paragraphs_from_pdf(pdf_path, use_columns=True, column_split=300):\n",
    "#     doc = fitz.open(pdf_path)\n",
    "#     paragraphs = []\n",
    "\n",
    "#     for page_num, page in enumerate(doc, start=1):\n",
    "#         text = extract_text_by_columns(page, column_split) if use_columns else page.get_text(\"text\")\n",
    "#         lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "\n",
    "#         para_buffer = \"\"\n",
    "#         para_num = 0\n",
    "\n",
    "#         for line in lines:\n",
    "#             para_buffer += ' ' + line.strip()\n",
    "\n",
    "#             if re.search(r'[.?!:;]$', line.strip()) or len(line.strip()) < 40:\n",
    "#                 if is_valid_paragraph(para_buffer):\n",
    "#                     para_num += 1\n",
    "#                     paragraphs.append({\n",
    "#                         'page': page_num,\n",
    "#                         'paragraph_number': para_num,\n",
    "#                         'text': para_buffer.strip(),\n",
    "#                         'word_count': len(para_buffer.strip().split())\n",
    "#                     })\n",
    "#                 para_buffer = \"\"\n",
    "\n",
    "#         if is_valid_paragraph(para_buffer):\n",
    "#             para_num += 1\n",
    "#             paragraphs.append({\n",
    "#                 'page': page_num,\n",
    "#                 'paragraph_number': para_num,\n",
    "#                 'text': para_buffer.strip(),\n",
    "#                 'word_count': len(para_buffer.strip().split())\n",
    "#             })\n",
    "\n",
    "#     return paragraphs\n",
    "\n",
    "# # Detect language of each paragraph\n",
    "# def detect_languages(paragraphs):\n",
    "#     lang_results = []\n",
    "#     for p in paragraphs:\n",
    "#         try:\n",
    "#             lang = detect(p['text'])\n",
    "#         except:\n",
    "#             lang = \"unknown\"\n",
    "#         p['language'] = lang\n",
    "#         lang_results.append(lang)\n",
    "#     return paragraphs, lang_results\n",
    "\n",
    "# # Identify the major language and extract foreign paragraphs\n",
    "# def find_foreign_paragraphs(paragraphs, lang_results):\n",
    "#     lang_count = Counter(lang_results)\n",
    "#     major_language = lang_count.most_common(1)[0][0]\n",
    "\n",
    "#     # Filter paragraphs that are not in the major language\n",
    "#     foreign_paragraphs = [\n",
    "#         p for p in paragraphs if p['language'] != major_language and p['language'] != \"unknown\"\n",
    "#     ]\n",
    "\n",
    "#     return major_language, foreign_paragraphs\n",
    "\n",
    "# # Main function to analyze PDF language\n",
    "# def analyze_pdf_language(pdf_path, use_columns=True, column_split=300):\n",
    "#     paragraphs = extract_paragraphs_from_pdf(pdf_path, use_columns, column_split)\n",
    "#     paragraphs, lang_results = detect_languages(paragraphs)\n",
    "#     major_language, foreign_paragraphs = find_foreign_paragraphs(paragraphs, lang_results)\n",
    "\n",
    "#     print(f\"\\nâœ… Major language detected: {major_language}\")\n",
    "#     print(f\"ðŸš¨ Foreign paragraphs found: {len(foreign_paragraphs)}\")\n",
    "\n",
    "#     # Create DataFrame for foreign paragraphs and all paragraphs\n",
    "#     df_foreign = pd.DataFrame(foreign_paragraphs)\n",
    "#     df_all = pd.DataFrame(paragraphs)\n",
    "\n",
    "#     return major_language, df_foreign, df_all\n",
    "\n",
    "# # Example Usage\n",
    "\n",
    "# major_language, df_foreign, df_all = analyze_pdf_language(pdf_path)\n",
    "\n",
    "# # Show the first few rows of the foreign paragraphs DataFrame\n",
    "# # print(df_foreign.head())\n",
    "\n",
    "# # Optionally, save the foreign language paragraphs DataFrame to CSV\n",
    "# # df_foreign.to_csv('foreign_paragraphs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# pdf_path = pdf_file\n",
    "# =======================\n",
    "# ðŸš€ Example usage\n",
    "# =======================\n",
    "\n",
    "# pdf_path = \"your_manual.pdf\"\n",
    "\n",
    "# # Enable column detection (for 2-column layout PDFs)\n",
    "# major_lang, foreign_df, all_df = analyze_pdf_language(pdf_path, use_columns=True, column_split=300)\n",
    "\n",
    "# # # Save to CSV\n",
    "# # foreign_df.to_csv(\"foreign_language_paragraphs.csv\", index=False)\n",
    "# # all_df.to_csv(\"all_paragraphs_with_wordcount.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langdetect import detect\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Function to validate a paragraph based on length and punctuation\n",
    "def is_valid_paragraph(text):\n",
    "    return (\n",
    "        len(text.split()) >= 8 and  # Minimum of 8 words\n",
    "        any(p in text for p in ['.', ':', ';', '!', '?'])  # End with punctuation\n",
    "    )\n",
    "\n",
    "# Function to handle multi-column layout\n",
    "def extract_text_by_columns(page, column_split=300):\n",
    "    blocks = page.get_text(\"blocks\")\n",
    "    left_col, right_col = [], []\n",
    "\n",
    "    for b in blocks:\n",
    "        x0, y0, x1, y1, text, *_ = b\n",
    "        if x0 < column_split:\n",
    "            left_col.append((y0, text))\n",
    "        else:\n",
    "            right_col.append((y0, text))\n",
    "\n",
    "    left_col.sort()\n",
    "    right_col.sort()\n",
    "    combined_text = '\\n'.join([t for _, t in left_col + right_col])\n",
    "    return combined_text\n",
    "\n",
    "# Extract paragraphs from the PDF\n",
    "def extract_paragraphs_from_pdf(pdf_path, use_columns=True, column_split=300):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    paragraphs = []\n",
    "\n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        text = extract_text_by_columns(page, column_split) if use_columns else page.get_text(\"text\")\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "\n",
    "        para_buffer = \"\"\n",
    "        para_num = 0\n",
    "\n",
    "        for line in lines:\n",
    "            para_buffer += ' ' + line.strip()\n",
    "\n",
    "            if re.search(r'[.?!:;]$', line.strip()) or len(line.strip()) < 40:\n",
    "                if is_valid_paragraph(para_buffer):\n",
    "                    para_num += 1\n",
    "                    paragraphs.append({\n",
    "                        'page': page_num,\n",
    "                        'paragraph_number': para_num,\n",
    "                        'text': para_buffer.strip(),\n",
    "                        'word_count': len(para_buffer.strip().split())\n",
    "                    })\n",
    "                para_buffer = \"\"\n",
    "\n",
    "        if is_valid_paragraph(para_buffer):\n",
    "            para_num += 1\n",
    "            paragraphs.append({\n",
    "                'page': page_num,\n",
    "                'paragraph_number': para_num,\n",
    "                'text': para_buffer.strip(),\n",
    "                'word_count': len(para_buffer.strip().split())\n",
    "            })\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "# Detect language of each paragraph\n",
    "def detect_languages(paragraphs):\n",
    "    lang_results = []\n",
    "    for p in paragraphs:\n",
    "        try:\n",
    "            lang = detect(p['text'])\n",
    "        except:\n",
    "            lang = \"unknown\"\n",
    "        p['language'] = lang\n",
    "        lang_results.append(lang)\n",
    "    return paragraphs, lang_results\n",
    "\n",
    "# Identify the major language and extract foreign paragraphs\n",
    "def find_foreign_paragraphs(paragraphs, lang_results):\n",
    "    lang_count = Counter(lang_results)\n",
    "    major_language = lang_count.most_common(1)[0][0]\n",
    "\n",
    "    # Filter paragraphs that are not in the major language\n",
    "    foreign_paragraphs = [\n",
    "        p for p in paragraphs if p['language'] != major_language and p['language'] != \"unknown\"\n",
    "    ]\n",
    "\n",
    "    return major_language, foreign_paragraphs\n",
    "\n",
    "# Main function to analyze PDF language\n",
    "def analyze_pdf_language(pdf_path, use_columns=True, column_split=300):\n",
    "    paragraphs = extract_paragraphs_from_pdf(pdf_path, use_columns, column_split)\n",
    "    paragraphs, lang_results = detect_languages(paragraphs)\n",
    "    major_language, foreign_paragraphs = find_foreign_paragraphs(paragraphs, lang_results)\n",
    "\n",
    "    print(f\"\\nâœ… Major language detected: {major_language}\")\n",
    "    print(f\"ðŸš¨ Foreign paragraphs found: {len(foreign_paragraphs)}\")\n",
    "\n",
    "    # Create DataFrame for foreign paragraphs and all paragraphs\n",
    "    df_foreign = pd.DataFrame(foreign_paragraphs)\n",
    "    df_all = pd.DataFrame(paragraphs)\n",
    "\n",
    "    return major_language, df_foreign, df_all, lang_results\n",
    "\n",
    "# Function to perform clustering based on language similarity\n",
    "def cluster_by_language(df_all, num_clusters=4):\n",
    "    # Use the 'language' column to create a similarity score for clustering\n",
    "    lang_count = df_all['language'].value_counts()\n",
    "    print(f\"\\nLanguage Distribution:\\n{lang_count}\")\n",
    "\n",
    "    # Vectorizing the language column directly to find patterns based on language\n",
    "    lang_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X_lang = lang_vectorizer.fit_transform(df_all['language'].fillna(\"unknown\"))  # Fill NaNs if any\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    df_all['cluster'] = kmeans.fit_predict(X_lang)\n",
    "\n",
    "    return df_all, kmeans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "\n",
    "major_language, df_foreign, df_all, lang_results = analyze_pdf_language(pdf_path)\n",
    "\n",
    "# Perform clustering based on language similarity\n",
    "df_all_clustered, kmeans_model = cluster_by_language(df_all, num_clusters=4)\n",
    "\n",
    "# Show the first few rows of the clustered DataFrame\n",
    "print(df_all_clustered.head())\n",
    "\n",
    "# Optionally, save the clustered DataFrame to CSV\n",
    "# df_all_clustered.to_csv('clustered_paragraphs_by_language.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_clustered.groupby('cluster')['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langdetect import detect\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def is_valid_paragraph(text):\n",
    "    \"\"\"Heuristic to validate a meaningful paragraph.\"\"\"\n",
    "    word_count = len(text.split())\n",
    "    has_punctuation = any(p in text for p in ['.', ':', ';', '!', '?'])\n",
    "    not_code_like = not re.match(r'^\\w{2,10}[-\\d]*$', text.strip())  # avoid codes\n",
    "    not_part_number = not re.search(r'\\b(part\\s*no|ref|code|item)\\b', text.lower())\n",
    "    not_dotted_line = not re.match(r'^.*\\.{4,}.*$', text)  # remove lines with many dots (TOC-style)\n",
    "    return word_count >= 8 and has_punctuation and not_code_like and not_part_number and not_dotted_line\n",
    "\n",
    "def extract_text_by_columns(page, column_split=300):\n",
    "    \"\"\"Extracts text block-wise preserving column layout.\"\"\"\n",
    "    blocks = page.get_text(\"blocks\")\n",
    "    left_col, right_col = [], []\n",
    "\n",
    "    for b in blocks:\n",
    "        x0, y0, x1, y1, text, *_ = b\n",
    "        if x0 < column_split:\n",
    "            left_col.append((y0, text))\n",
    "        else:\n",
    "            right_col.append((y0, text))\n",
    "\n",
    "    left_col.sort()\n",
    "    right_col.sort()\n",
    "    combined_text = '\\n'.join([t for _, t in left_col + right_col])\n",
    "    return combined_text\n",
    "\n",
    "def clean_line(line):\n",
    "    \"\"\"Basic line hygiene.\"\"\"\n",
    "    line = line.strip()\n",
    "    if not line or line.count('.') > 10:  # eliminate TOC-style lines\n",
    "        return ''\n",
    "    if re.fullmatch(r'[-â€“â€”_\\s\\d.]+', line):  # dashes, dots, whitespace only\n",
    "        return ''\n",
    "    return line\n",
    "\n",
    "def extract_paragraphs_from_pdf(pdf_path, use_columns=True, column_split=300):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    paragraphs = []\n",
    "\n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        text = extract_text_by_columns(page, column_split) if use_columns else page.get_text(\"text\")\n",
    "        lines = [clean_line(line) for line in text.split('\\n')]\n",
    "        lines = [line for line in lines if line]\n",
    "\n",
    "        para_buffer = \"\"\n",
    "        para_num = 0\n",
    "\n",
    "        for line in lines:\n",
    "            if para_buffer:\n",
    "                para_buffer += ' ' + line.strip()\n",
    "            else:\n",
    "                para_buffer = line.strip()\n",
    "\n",
    "            if re.search(r'[.?!:;]$', line.strip()) or len(line.strip()) < 40:\n",
    "                if is_valid_paragraph(para_buffer):\n",
    "                    para_num += 1\n",
    "                    paragraphs.append({\n",
    "                        'page': page_num,\n",
    "                        'paragraph_number': para_num,\n",
    "                        'text': para_buffer.strip(),\n",
    "                        'word_count': len(para_buffer.strip().split())\n",
    "                    })\n",
    "                para_buffer = \"\"\n",
    "\n",
    "        if is_valid_paragraph(para_buffer):\n",
    "            para_num += 1\n",
    "            paragraphs.append({\n",
    "                'page': page_num,\n",
    "                'paragraph_number': para_num,\n",
    "                'text': para_buffer.strip(),\n",
    "                'word_count': len(para_buffer.strip().split())\n",
    "            })\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "def detect_languages(paragraphs):\n",
    "    lang_results = []\n",
    "    for p in paragraphs:\n",
    "        try:\n",
    "            lang = detect(p['text'])\n",
    "        except:\n",
    "            lang = \"unknown\"\n",
    "        p['language'] = lang\n",
    "        lang_results.append(lang)\n",
    "    return paragraphs, lang_results\n",
    "\n",
    "def find_foreign_paragraphs(paragraphs, lang_results):\n",
    "    lang_count = Counter(lang_results)\n",
    "    major_language = lang_count.most_common(1)[0][0]\n",
    "\n",
    "    foreign_paragraphs = [\n",
    "        p for p in paragraphs if p['language'] != major_language and p['language'] != \"unknown\"\n",
    "    ]\n",
    "\n",
    "    return major_language, foreign_paragraphs\n",
    "\n",
    "def analyze_pdf_language(pdf_path, use_columns=True, column_split=300):\n",
    "    paragraphs = extract_paragraphs_from_pdf(pdf_path, use_columns, column_split)\n",
    "    paragraphs, lang_results = detect_languages(paragraphs)\n",
    "    major_language, foreign_paragraphs = find_foreign_paragraphs(paragraphs, lang_results)\n",
    "\n",
    "    print(f\"\\nâœ… Major language detected: {major_language}\")\n",
    "    print(f\"ðŸš¨ Foreign paragraphs found: {len(foreign_paragraphs)}\")\n",
    "\n",
    "    df_foreign = pd.DataFrame(foreign_paragraphs)\n",
    "    df_all = pd.DataFrame(paragraphs)\n",
    "    return major_language, df_foreign, df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Major language detected: bg\n",
      "ðŸš¨ Foreign paragraphs found: 8\n"
     ]
    }
   ],
   "source": [
    "major_language, df_foreign, df_all=analyze_pdf_language(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
